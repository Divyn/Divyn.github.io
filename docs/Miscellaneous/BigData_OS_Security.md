---
layout: default
title: Big Data Analytics and Autonomic Computing in Anticipating Operating System Threats
parent: Miscellaneous
---

# Big Data Analytics and Autonomic Computing in Anticipating Operating System Threats

 A major portion of world’s population now has Internet access with the number of devices connected currently in billions. The decline in the cost of hardware resources, especially the cost of storage devices is one factor that enables organisations  to  store  every  bit  of  data  leading  to  the accumulation of data in volumes unimaginable.Here traditional network  perimeter  defence  systems  become  ineffective  in resisting targeted attacks as they are not scalable to the size of organizational networks.

## INTRODUCTION

The operating system(OS) manages resources within the system. It determines the allocation of these resources to the multiple tasks and allow them access in an orderly manner.An OS is large and  having complex programs therefore security issues will be present.Security issues include the external environment of the system and  the need to protect it from unauthorized access and malicious modification or destruction and inconsistency in operation.The OS must protect itself from security breaches including denial of service, memory- access violations, stack overflow violations, the launching of programs with too many privileges and many others. Having the right tools to organise and interpret data  is also important.By collecting data about users’ and hosts’ activities within the organization’s network collected by firewalls, web proxies, domain controllers, intrusion detection systems and Virtual Private Network (VPN) servers’ which contain information about system behaviour in various situations that can be used for detecting stealthy attacks.Big data not only helps to make data visible only to authorized entities but also offers the transmission of data at higher rate due to the advanced type of distributed file system. Internet connected sensors and devices let to massive increase in number of streams.Cloud computing provides external scalable processing capabilities and vast amounts of storage required for big data.

## BIG DATA IN AUTONOMIC COMPUTING

As far as computing world is concerned systems are being  programmed to become self-managing ,i.e. move towards autonomic computing. The system makes decisions on its own, using high-level policies; it will constantly check and optimize its status and automatically adapt itself to changing conditions. An autonomic application should be able to detect suboptimal processes and optimize itself to improve its execution.

Big data plays a major role in these requirements.The  technological advances in storage, processing and analysis of big data includes the rapidly decreasing cost of storage,the development of new frameworks such as Hadoop and the  flexibility and cost-effectiveness of cloud computing .These features enables the  analyst to predict the possible behaviour from a large set of possible behaviours  to counter any change in  the system . 

Advanced Persistent Threats(APT) occur in multiple stages .Therefore each action by the attacker provides an opportunity to detect behavioural deviations from the ideal. Correlating these apparently independent events can reveal evidence of the intrusion and expose stealthy attacks that could not be identified using the previous methods.  After seperation into clusters,big data as such does not reveal anything.A human analyst still has to interpret any result.For example, malicious domains tend to last for a short time, whereas good domains last much longer and resolve to many geographically-distributed Internet Protocol (IP) addresses’.The data derived from domain names including Domain Name Server responses (also called resource records) should be collected. Then, classification techniques can be used to identify infected hosts and malicious domains. This can be combined with the malicious sites identified from Internet Service provider data set .The incorporation of meaningful meta-data attributes into semi-structured data and unstructured content for big data makes these data assets more valuable whereby irrelevant information can be dismissed during the search process[8].

## METADATA IN ANALYTICS

When we apply search algorithms to meta data, we will be able to create high level confidence results. This is particularly beneficial in analysing the problems related to the operating system since unwanted information need not be stored. With this meta data association, big data scientists can quickly locate the right information despite the vast amount of content stored  in  these  repositories.Utilizing  these  additional capabilities can lead to improved data handling and overall big data - meta data efficiency.Information about source of data and how it has been processed helps determine the authencity and quality of data (provenance).Whenever big data changes state, provenance information that is received is recorded as meta data.



## THEORY AND BIG DATA

Batch processing(offline processing) involves processing data in batches when large quantities of data are involved.It is implemented using MapReduce algorithms.The downside of using big data is that  it may introduce new problems like multiple comparisons. John Ioannidis well known for his paper  "[Why Most Published Research Findings Are False ](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/)" talks about essentially the same effect: when many researchers process a big amount of scientific data although not with big data technology,the likelihood of a "significant" result being actually false grows fast .

Francis Bacon ,a philosopher who preferred an empirical, inductive approach believed that “foundation of a true philosophy” was a storehouse of facts and is the  “[natural and experimental history](http://www.constitution.org/bacon/preparative.htm)” of all the relevant information. But he also believed that theories should look into physical causes that triggered the  observations so as to predict the results of observations  not included in the data that generated the theory.The only basis for increasing accuracy is to have a model as a means to judge its quality. Theories decide the context in which data must be considered , especially when it comes to retaining and preserving data for further usage. One such example is given below:
Strong ground motion is modeled by recording time-series files at the “surface” of the simulated volume. Various parameters describing shaking can be calculated from these time series, such as peak ground acceleration and duration of shaking. These simulated quantities can be used to evaluate the seismic hazard in a region and building codes can be modified accordingly [5].

The Southern California Earthquake Center developed a very large-scale simulation project called TeraShake in order to evaluate the threat from earthquakes on the southern San Andreas fault. TeraShake uses a supercomputer with 240 processors to solve wave equations in a 600 kilometer by 300 kilometer by 80 kilometer volume with 200 meter resolution. A simulation lasting three minutes generates almost 50 Terabytes of output. Scientists use this data to conduct further research into understanding the earthquake process[5]. 

Big data analysis blends traditional statistical analysis approaches with computational ones. Once diagnostic analysis using statistical methods is executed computational techniques in big data can be applied.Big data can shift from batch processing to real time processing.In the long run,an organisation will operate its big data analysis at two levels:processing streaming data as it arrives and performing batch analysis of data as it accumulates to look for patterns and trends.

## BIG DATA AND SECURITY

It cannot be denied that managing Big Data systems  involves various security issues. New resource management and scheduling mechanisms are also needed to utilize the tools including the need to create  platform layers. Several open source and proprietary solutions have been proposed to address these requirements.But its employability for data accumulation rate outweigh the liabilities. It can be used to correlate events across time and space. Reporting and visualization of predicted outcomes can be done effectively.Big data analysis systems, such as MapReduce and Spark, address the computational needs in security analysis as they are highly resistant to failure. Since security events generate so much data, there is a risk of overwhelming analysts and also limits their ability to discern key events. Useful big data tools frame data in the context of users, devices and events.Big data analytics will collect data from any device that is connected to an IP network via the Internet. This includes anything from laptops and smartphones to Internet of Things devices. In addition to physical devices and virtual servers, big data security analytics can attend to software-related security. 

## CONCLUSION

We can assess our network, scan it for vulnerabilities, prioritize the measures to understand how things are configured and then discover where the error occurs.

Big data is used for deciding business strategies which deals with live and dynamic subjects(people).Likewise computer security analysis indirectly deals with dynamic subjects. 

Big data can be used to carry out predictive analysis on how personal systems work under threats and  incorporate self resurrecting programs to solve minor issues without connecting to the manufacturer .A large organisation  must have a  team in the IT department dedicated to analysing the data transmission happening within the organisation using big data tools.It can be used to prevent an attack on companies carrying out critical business operations with a consequent impact on customer service. Data leaks can lead to loss of confidential data. The technical mechanisms developed for efficient computation and storage in the field of big data has received less attention and  development in this area can lead to significant strengthening of security .

### References

1. Big data fundamentals by Thomas Erl,Wajid Khattak,Paul Buhler
1. Computer security by David K. Hsiao, Douglas S. Kerr, Stuart E. Madnick 
1. Towards a Big Data Architecture for Facilitating Cyber Threat Intelligence:Charles Wheelus, Elias Bou-Harb, Xingquan Zhu
1. <http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf>
1. <http://www.siam.org/careers/pdf/earthquake.pdf>
1. [http://www.datasciencecentral.com/profiles/blogs/why-you-need- metadata-for-big-data-success](http://www.datasciencecentral.com/profiles/blogs/why-you-need-metadata-for-big-data-success)
1. [https://securityintelligence.com/the-use-case-for-big-data-and-security- analytics-an-interview-with-ben-wuest/](https://securityintelligence.com/the-use-case-for-big-data-and-security-analytics-an-interview-with-ben-wuest/)
1. [https://downloads.cloudsecurityalliance.org/initiatives/bdwg/Big_Data _Analytics_for_Security_Intelligence.pdf](https://downloads.cloudsecurityalliance.org/initiatives/bdwg/Big_Data_Analytics_for_Security_Intelligence.pdf)
1. [http://www.valuewalk.com/wp-content/uploads/2016/12/Big-Data-In- Practice-From-Buzz-Word-To-Business-Benefits.jpg](http://www.valuewalk.com/wp-content/uploads/2016/12/Big-Data-In-Practice-From-Buzz-Word-To-Business-Benefits.jpg)
1. <https://hbr.org/2012/04/good-data-wont-guarantee-good-decisions>